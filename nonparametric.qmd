---
title: "Non Parametric and Counting Statistics"
author: "Rebecca Hoover"
title-block-banner: true
date: "12/03/2024"
abstract: "Counting and non-parametric statistics play a crucial role when dealing with categorical data for both predictors and response variables, or when the assumptions required for parametric methods (such as normality, homoscedasticity, etc.) are not met. These methods offer alternative approaches to analyzing data that may not conform to the assumptions of traditional parametric techniques like correlation, regression, or ANOVA, providing robust tools for data analysis in non-ideal situations."
smooth-scroll: true
format: html
editor: visual
editor_options: 
  chunk_output_type: console
---

## Student Learning Objectives

At the end of this topic, you should be able to: 

-   Identify specific non-parametric approaches that could be used based on the kind of data being presented. 

-   Perform appropriate non-parametric analyses given the raw data.

-   Communicate the results of non-parametric analyses. 


For each of the following questions, please provide your analysis and an interpretation (e.g., written as you would in a scientific publication). If it helps to describe your result, add tables or figures to help make your case. For every case, explain why you chose the particular analysis you did and demonstrate the reasons from the data.

***All Figures are located in Appendix at the Bottom of this HTML***

### Question 1.  

The FDA has suggested a change in a medication that has been shown to have detrimental side effects in half of the patients. A clinical trial was conducted with nineteen patients; only three reported side effects. Did the change make a significant difference? 


*Null Hypothesis (H₀)*: The proportion of patients who experience side effects after the change is the same as it was before the change (which we assume is 50%, or 0.5, based on the information that half of the patients experienced side effects before).

H0: p = 0.5

*Alternative Hypothesis (H₁)*: The proportion of patients who experience side effects after the change is different from 0.5.

H1: p ≠ 0.5


```{r}
# Define the observed values
n <- 19  
x <- 3   
p0 <- 0.5  

# Calculate the observed proportion
p_hat <- x / n

# Calculate the Z-test statistic
Z <- (p_hat - p0) / sqrt(p0 * (1 - p0) / n)

# Calculate the p-value for a two-tailed test
p_value <- 2 * (1 - pnorm(abs(Z)))

# Output the results
cat("Z-test statistic:", Z, "\n")
cat("P-value:", p_value, "\n")

# Check if the result is significant at alpha = 0.05
if (p_value < 0.05) {
  cat("The result is statistically significant. Reject the null hypothesis.\n")
} else {
  cat("The result is not statistically significant. Fail to reject the null hypothesis.\n")
}

```

**This statement verifies that the alteration in the medication resulted in a statistically significant effect on the observed outcomes. The analysis indicated a p-value that is lower than  0.05, suggesting that the likelihood of the observed results occurring by chance is extremely low. This strong statistical evidence supports the conclusion that the change in medication had a meaningful impact on the study's findings.**


----------------------------------------------------------------------------------------------

### Question 2.  

Two different environmental remediation treatments are evaluated for the impacts on turbidity (measured using the Nephelometric Turbidity Unit or NTU). For regulatory reasons, turbidity is binned into four groups based on NTU: 0-15 ntu, 16-22 ntu, 23-30 ntu, and 31+ ntu. Do both treatments have the same impact on turbidity? Explain.

```{r, message=FALSE, warning=FALSE}
# Load packages
library(dplyr)

# Load the data from CSV
ntu_data <- read.csv("ntu_data.csv")

# Create a new factor variable for turbidity bins
ntu_data$Turbidity_Binned <- cut(ntu_data$NTU,
                                  breaks = c(0, 15, 22, 30, Inf),
                                  labels = c("0-15", "16-22", "23-30", "31+"),
                                  right = TRUE)

# Create a contingency table
contingency_table <- table(ntu_data$Treatment, ntu_data$Turbidity_Binned)

# Perform the Chi-Square test
chi_square_result <- chisq.test(contingency_table)

# View the results of the Chi-Square test
chi_square_result

# Visualizing the data with a bar plot
library(ggplot2)

vis1 <- ggplot(ntu_data, aes(x = Treatment, 
                     fill = Turbidity_Binned)) +
  geom_bar(position = "stack", 
           color = "black", 
           size = 0.5) +
  labs(title = "Turbidity Distribution by Treatment",
       x = "Treatment",
       y = "Frequency",
       fill = "NTU Binned")+
  scale_fill_brewer(palette = "Set3")


```

**In this study, turbidity levels were categorized into four distinct bins: 0-15 NTU, 16-22 NTU, 23-30 NTU, and 31+ NTU. A contingency table was constructed to assess how frequently each treatment resulted in turbidity within these categories. To evaluate the potential differences in turbidity effects between the treatments, a chi-square test was conducted.**
**The results of the analysis yielded a p-value of 0.0006, which is significantly lower than the 0.05 threshold. This finding suggests a statistically significant difference in the turbidity outcomes associated with the two treatments.**
**Further examination reveals that Treatment B is associated with a higher frequency of turbidity across the various bins, indicating that it contributes more significantly to increased turbidity levels. In contrast, Treatment A appears to correlate with lower turbidity frequencies, resulting in less pronounced separations within the established categories. These outcomes underscore the distinct impacts of each treatment on turbidity levels, with Treatment B leading to a more considerable increase in turbidity compared to Treatment A.**

----------------------------------------------------------------------------------------------

### Question 3.  

A dozen graduate students tried to determine if there was a relationship between their undergraduate GPA and their scores on the Graduate Records Examination. Look at these data and determine the extent to which they are related. Explain.

```{r, message=FALSE, warning=FALSE}

# Read the CSV data
data <- read.csv("grad_school.csv")

# Scatter plot
vis2 <- ggplot(data, aes(x = GPA, y = GRE)) +
  geom_point(shape = 19) +
  labs(
    title = "Scatter plot of GPA vs. GRE",
    x = "Undergraduate GPA",
    y = "GRE Score"
  )+
  theme_minimal()

# Spearman Correlation
spearman_corr <- cor.test(data$GPA, data$GRE, method = "spearman")

print(spearman_corr)


```

**In this study, a Spearman correlation test was conducted to explore the relationship between Grade Point Average (GPA) and Graduate Record Examination (GRE) scores, chosen for its suitability in analyzing monotonic relationships without assuming linearity. The results yielded a Spearman correlation coefficient of 0.585 and a p-value of 0.046, indicating a moderate positive correlation that approaches statistical significance. These findings suggest that students with higher GPAs tend to achieve better GRE scores, reflecting a positive association between undergraduate academic performance and GRE performance. However, the correlation is not perfect, as illustrated by the scatterplot, which shows a slight positive linear trend accompanied by some variability in data points. This variability indicates that other factors may also influence GRE scores among students with comparable GPAs, emphasizing the complexity of academic assessment and the need for further research to understand these underlying dynamics.**

----------------------------------------------------------------------------------------------

### Question 4.  

You are looking at fruit yield on dogwood. You designed an experiment with four different treatments and measured the total yield in germinated seeds. Are there differences in yield? Explain.

```{r, message=FALSE, warning=FALSE}

# Load the data
data_dog <- read.csv("DogwoodSeeds.csv")

# Kruskal-Wallis Test
kruskal_test <- kruskal.test(Seeds ~ Treatment, data = data_dog)
print(kruskal_test)

# Visualize the boxplot with ggplot2
library(ggplot2)

vis3 <- ggplot(data_dog, aes(x = Treatment, 
                         y = Seeds, 
                         fill = Treatment)) +
  geom_boxplot() +
  labs(title = "Boxplot of Seed Yields by Treatment", 
       x = "Treatment", 
       y = "Seed Yield") +
   scale_fill_brewer(palette = "Set3") +
  theme_minimal() +
  theme(legend.position = "none")

```

**In this study, the Kruskal-Wallis test was employed to compare seed yield across four different treatments, as preliminary analysis indicated that the data may not be normally distributed, a conclusion supported by the box and whisker plot illustrating the non-comparability of the treatments. The results yielded a p-value of 0.00001, which is less than the 0.05 threshold, indicating a statistically significant difference in seed yield among the treatments. This significant finding, coupled with the observed high variation in the boxplot, suggests that the treatments have a meaningful impact on seed yield, warranting further investigation into their specific effects.**

-----------------------------------------------------------------------------

# Appendix A : Tables and Figures

```{r, message=FALSE, warning=FALSE}
library(gridExtra)

grid.arrange(vis1,
             vis2,
             vis3,
             ncol = 2)
```